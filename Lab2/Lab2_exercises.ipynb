{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bfd2e21-eac6-485b-8f36-0a7285d371ed",
   "metadata": {},
   "source": [
    "<b><u>Problem 1:</u></b>\n",
    "\n",
    "Work out the gradient <span style=\"font-size:18px;\">$ \\frac{dz}{da} $ </span> and compare the result with the analytical gradient.\n",
    "$$ \n",
    "x = 2 \\cdot a + 3 \\cdot b \n",
    "$$\n",
    "$$\n",
    "y = 5 \\cdot a^2 + 3 \\cdot b^3\n",
    "$$\n",
    "$$\n",
    "z = 2 \\cdot x + 3 \\cdot y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "868034bb-592c-4537-9d3e-61172a5fcbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Gradient dz/da: 64.0\n",
      "Analytical Gradient dz/da: 64.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a, b as tensors with requires_grad=True for gradient computation\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(1.0)\n",
    "\n",
    "# Define the function x, y, and z\n",
    "x = 2*a + 3*b\n",
    "y = 5*a**2 + 3*b**3\n",
    "z = 2*x + 3*y\n",
    "\n",
    "# Compute the gradient dz/da\n",
    "z.backward()\n",
    "\n",
    "# Print the computed gradient\n",
    "print(\"Computed Gradient dz/da:\", a.grad.item())\n",
    "\n",
    "# Analytical gradient for comparison\n",
    "analytical_gradient = 4 + 30 * a.item()\n",
    "print(\"Analytical Gradient dz/da:\", analytical_gradient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c9582a-e6f1-466a-bd3b-46ee962da333",
   "metadata": {},
   "source": [
    "<b><u>Problem 2:</u></b>\n",
    "\n",
    "Work out the gradient <span style=\"font-size:18px;\">$ \\frac{da}{dw} $ </span> for a = $ \\sigma(wx+b) $and compare the result with the analytical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa753b45-d3c2-45e0-a728-fb667f1b17f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Gradient da/dw: 0.0701037123799324\n",
      "Analytical Gradient da/dw: 0.07010371292573936\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define w, x, b as tensors with requires_grad=True for gradient computation\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "x = torch.tensor(1.0)\n",
    "b = torch.tensor(0.5)\n",
    "\n",
    "# Define the sigmoid function and compute a = sigmoid(wx + b)\n",
    "a = torch.sigmoid(w * x + b)\n",
    "\n",
    "# Compute the gradient da/dw\n",
    "a.backward()\n",
    "\n",
    "# Print the computed gradient\n",
    "print(\"Computed Gradient da/dw:\", w.grad.item())\n",
    "\n",
    "# Analytical gradient for comparison\n",
    "sigmoid_value = torch.sigmoid(w * x + b).item()  # Calculate sigmoid(wx + b)\n",
    "analytical_gradient = sigmoid_value * (1 - sigmoid_value) * x.item()\n",
    "print(\"Analytical Gradient da/dw:\", analytical_gradient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0460b4df-226c-41ae-a924-5c22fdd077f4",
   "metadata": {},
   "source": [
    "<b><u>Problem 3:</u></b>\n",
    "\n",
    "Verify that the gradients provided by PyTorch match with the analytical gradients of the function $$\n",
    "f(x) = \\exp(-x^2 - 2x - \\sin(x))\n",
    "$$\n",
    "w.r.t $ x $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7758c419-3dea-4273-9fed-ec0a16c04ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Gradient df/dx: -0.09744400531053543\n",
      "Analytical Gradient df/dx: -0.09744400531053543\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define x as a tensor with requires_grad=True\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# Define the function f(x) = exp(-x^2 - 2x - sin(x))\n",
    "f = torch.exp(-x**2 - 2*x - torch.sin(x))\n",
    "\n",
    "# Compute the gradient df/dx\n",
    "f.backward()\n",
    "\n",
    "# Print the computed gradient\n",
    "print(\"Computed Gradient df/dx:\", x.grad.item())\n",
    "\n",
    "# Analytical gradient for comparison\n",
    "import math\n",
    "analytical_gradient = torch.exp(-x**2 - 2*x - torch.sin(x)) * (-2*x - 2 - torch.cos(x))\n",
    "print(\"Analytical Gradient df/dx:\", analytical_gradient.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589cc52f-ef97-4d45-af9f-2cabe206b8d2",
   "metadata": {},
   "source": [
    "<b><u>Problem 4:</u></b>\n",
    "\n",
    "For the following function, calculate $ \\frac{\\partial f}{\\partial x} $  and $ \\frac{\\partial f}{\\partial y} $ using the computational graph and chain rule. Use the chain rule to calculate gradient and compare with analytical gradient.\n",
    "\n",
    "$$ f(x,y,z) = tanh(ln[1 + z \\cdot \\frac{2x}{\\sin(y)} ]) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a176328a-c6de-4b3b-93a2-189e94ea1d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Gradient df/dx: 0.20869959890842438\n",
      "Computed Gradient df/dy: -0.13400448858737946\n",
      "Analytical Gradient df/dx: 0.06188088655471802\n",
      "Analytical Gradient df/dy: -0.03973326086997986\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define x, y, z as tensors with requires_grad=True for gradient computation\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(1.0, requires_grad=True)\n",
    "z = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# Define the function f(x, y, z)\n",
    "u = (2 * x) / torch.sin(y)\n",
    "v = z * u\n",
    "w = 1 + v\n",
    "t = torch.log(w)\n",
    "f = torch.tanh(t)\n",
    "\n",
    "# Compute the gradients df/dx and df/dy\n",
    "f.backward()\n",
    "\n",
    "# Print the computed gradients\n",
    "print(\"Computed Gradient df/dx:\", x.grad.item())\n",
    "print(\"Computed Gradient df/dy:\", y.grad.item())\n",
    "\n",
    "# Now calculate the analytical gradients (as derived above)\n",
    "sech_t = 1 - torch.tanh(t)**2  # Computing sech(t)\n",
    "analytical_df_dx = sech_t**2 * (1 / w) * z * (2 / torch.sin(y))\n",
    "analytical_df_dy = sech_t**2 * (1 / w) * z * (-2 * x * torch.cos(y) / torch.sin(y)**2)\n",
    "\n",
    "print(\"Analytical Gradient df/dx:\", analytical_df_dx.item())\n",
    "print(\"Analytical Gradient df/dy:\", analytical_df_dy.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e3d7c-16e5-4572-ba73-1339280ab99f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
