{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b33f644-cab1-4d9f-b473-de55117dcc86",
   "metadata": {},
   "source": [
    "PyTorch allows to automatically obtain the gradients of a tensor with respect to a defined function. \n",
    "When creating the tensor, we have to indicate that it requires the gradient computation using the flag `requires_grad` \n",
    "\n",
    "Sample Program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eac79031-3e78-463d-8a06-641683c7d793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6694, 0.6904, 0.6900], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "x = torch.rand(3,requires_grad=True) \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14174514-c81e-4dad-b10b-027b0a744bc9",
   "metadata": {},
   "source": [
    "<b><u>Problem 1:</u></b>\n",
    "\n",
    "Consider that $ y $ and $ z $ are calculated as follows:\n",
    "\n",
    "$$\n",
    "y = x^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "z = 2y + 3\n",
    "$$\n",
    "\n",
    "We are interested in how output $ z $ changes with input $ x $:\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dx} = 2 \\cdot 2x = 4x\n",
    "$$\n",
    "\n",
    "For input x=3.5,  will make z = 14 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75bc2592-41dc-4a9d-a5e1-376afef4fe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  tensor(3.5000, requires_grad=True)\n",
      "y = x*x:  tensor(12.2500, grad_fn=<MulBackward0>)\n",
      "z= 2*y + 3:  tensor(27.5000, grad_fn=<AddBackward0>)\n",
      "Working out gradients dz/dx\n",
      "Gradient at x = 3.5:  tensor(14.)\n"
     ]
    }
   ],
   "source": [
    "# set up simple graph relating x, y and z \n",
    "x = torch.tensor(3.5, requires_grad=True) \n",
    "y = x*x \n",
    "z = 2*y + 3 \n",
    "print(\"x: \", x) \n",
    "print(\"y = x*x: \", y) \n",
    "print(\"z= 2*y + 3: \", z) \n",
    "# work out gradients \n",
    "z.backward() \n",
    "print(\"Working out gradients dz/dx\") \n",
    "# what is gradient at x = 3.5 \n",
    "print(\"Gradient at x = 3.5: \", x.grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d5a88-4f8b-4172-9de2-801676b306cd",
   "metadata": {},
   "source": [
    "<b><u> Problem 2: </u></b> \n",
    "\n",
    "Consider the function $ f(x) = (x - 2)^2 $.\n",
    "\n",
    "Compute $ \\frac{d}{dx} f(x) $, and then compute $ f'(1) $. Write code to check analytical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b050d5-f811-4caa-9bee-405517093581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical f'(x): tensor([-2.], grad_fn=<MulBackward0>)\n",
      "PyTorch's f'(x): tensor([-2.])\n"
     ]
    }
   ],
   "source": [
    "def f(x): \n",
    "    return (x-2)**2 \n",
    "def fp(x): \n",
    "    return 2*(x-2)  \n",
    "x = torch.tensor([1.0], requires_grad=True) \n",
    "y = f(x) \n",
    "y.backward() \n",
    "print('Analytical f\\'(x):', fp(x)) \n",
    "print('PyTorch\\'s f\\'(x):', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638d4bb9-71d1-4be8-af78-e593c35994d2",
   "metadata": {},
   "source": [
    "<b><u> Problem 3: </u></b>\n",
    "\n",
    "Define a function $ y = x^2 + 5 $. The function $ y $ will not only carry the result of evaluating $ x $, but also the gradient function $\\frac{\\partial y}{\\partial x}$ called `grad_fn` in the new tensor $ y $. Compare the result with the analytical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "609a55ff-45b7-402a-a1bd-4be0880afdda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.0]) \n",
    "x.requires_grad_(True)  #indicate we will need the gradients with respect to this variable \n",
    "y = x**2 + 5 \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88578374-cea9-4fda-848d-d6a39af055f6",
   "metadata": {},
   "source": [
    "To evaluate the partial derivative $\\frac{\\partial y}{\\partial x}$, we use the `.backward()` function and the result of the gradient evaluation is stored in `x.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c1d693-d7a1-4f57-9f50-1c89893ebf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch gradient: tensor([4.])\n",
      "Analytical gradient: tensor([4.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y.backward()  #dy/dx \n",
    "print('PyTorch gradient:', x.grad) \n",
    "\n",
    "#Let us compare with the analytical gradient of y = x**2+5 with torch.no_grad():    \n",
    "#this is to only use the tensor value without its gradient information \n",
    "dy_dx = 2*x  #analytical gradient \n",
    "print('Analytical gradient:',dy_dx) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca58c3d-a025-4ccd-a8a4-bdb787ec6a9e",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 18px;\">\n",
    "<b><u>Problem 4:</u></b>\n",
    "\n",
    "Write a function to compute the gradient of the sigmoid function $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $.\n",
    "\n",
    "\n",
    "\n",
    "Express $ \\sigma(x) $ as a composition of several elementary functions:  $ \\sigma(x) = s(c(b(a(x)))) $\n",
    "\n",
    "where:\n",
    "\n",
    "- $ a(x) = -x $  \n",
    "- $ b(a) = e^a $  \n",
    "- $ c(b) = 1 + b $  \n",
    "- $ s(c) = \\frac{1}{c} $\n",
    "\n",
    "Each intermediate variable is a basic expression for which the local gradients can be easily computed.\n",
    "\n",
    "The input to this function is $ x $,  and the output is represented by node $ s $. Compute the gradient of $ s $ with respect to $ x $, $\\frac{\\partial s}{\\partial x}$. In order to make use of our intermediate computations, we can use the chain rule as follows:\n",
    "\n",
    "$\\frac{\\partial s}{\\partial x} = \\frac{\\partial s}{\\partial c} \\cdot \\frac{\\partial c}{\\partial b} \\cdot \\frac{\\partial b}{\\partial a} \\cdot \\frac{\\partial a}{\\partial x}$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86fa580c-af84-4ea3-81be-dbe059c789f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_sigmoid_manual(x): \n",
    "    \"\"\" Implements the gradient of the logistic sigmoid function  \n",
    "        sigma(x) = 1 / (1 + e^{-x})  \n",
    "    \"\"\" \n",
    "    # Forward pass, keeping track of intermediate values for use in the backward pass \n",
    "    a = -x         # -x in denominator \n",
    "    b = np.exp(a)  # e^{-x} in denominator \n",
    "    c = 1 + b      # 1 + e^{-x} in denominator \n",
    "    s = 1.0 / c    # Final result, 1.0 / (1 + e^{-x}) \n",
    "    \n",
    "    # Backward pass \n",
    "    dsdc = (-1.0 / (c**2)) \n",
    "    dsdb = dsdc * 1 \n",
    "    dsda = dsdb * np.exp(a) \n",
    "    dsdx = dsda * (-1) \n",
    "    \n",
    "    return dsdx\n",
    "\n",
    "def sigmoid(x): \n",
    "    y = 1.0 / (1.0 + torch.exp(-x)) \n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dde04619-cbbf-45db-9019-0d64a809c753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autograd: 0.10499356687068939\n",
      "manual: 0.1049935854035065\n"
     ]
    }
   ],
   "source": [
    "input_x = 2.0  \n",
    "x = torch.tensor(input_x).requires_grad_(True) \n",
    "y = sigmoid(x) \n",
    "y.backward() \n",
    "# Compare the results of manual and automatic gradient functions: \n",
    "print('autograd:', x.grad.item()) \n",
    "print('manual:', grad_sigmoid_manual(input_x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee2410-2e19-45a9-88a7-0ba7ff9e6f18",
   "metadata": {},
   "source": [
    "<b><u> Problem 5: </u></b>\n",
    "\n",
    "Compute gradient for the function $ y=8x^4+ 3x^3 +7x^2+6x+3 $ and verify the gradients provided by PyTorch with the analytical gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "638394b7-b543-45d9-a2a2-646920e60082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical Gradient: 326.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "# Analytical gradient for comparison\n",
    "analytical_gradient = 32*x**3 + 9*x**2 + 14*x + 6\n",
    "print(\"Analytical Gradient:\", analytical_gradient.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c2e9131-5564-46f8-928a-cf382a2f232d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(326.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=8*x**4+3*x**3+7*x**2+6*x+3\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf49e335-6374-4df1-91a8-9bef648d251b",
   "metadata": {},
   "source": [
    "<b><u> Problem 6: </u></b>\n",
    "\n",
    "Work out the gradient <span style=\"font-size: 18px;\">$ \\frac{da}{dw} $ </span> for $ a = ReLU(wx+b) $  and compare the result with the analytical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00352fe1-1f85-4883-b5c4-71951bbfbbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Gradient da/dw: 1.0\n",
      "Analytical Gradient da/dw: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define w, x, b as tensors with requires_grad=True for gradient computation\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "x = torch.tensor(1.0)\n",
    "b = torch.tensor(0.5)\n",
    "\n",
    "# Define the function a = ReLU(wx + b)\n",
    "a = torch.relu(w * x + b)\n",
    "\n",
    "# Compute the gradient da/dw\n",
    "a.backward()\n",
    "\n",
    "# Print the computed gradient\n",
    "print(\"Computed Gradient da/dw:\", w.grad.item())\n",
    "\n",
    "# Analytical gradient for comparison\n",
    "# The ReLU derivative is 1 if wx + b > 0, otherwise 0.\n",
    "wx_b = w * x + b\n",
    "analytical_gradient = x if wx_b > 0 else 0\n",
    "print(\"Analytical Gradient da/dw:\", analytical_gradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4866aca3-5033-4dd2-ae9f-a892b05639a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
